import h5py
import numpy as np

import pycuda.driver as cuda
import pycuda.gpuarray as garray
from .BaseInputProcessor import BaseInputProcessor


class FileInputProcessor(BaseInputProcessor):
    """
    Parameters
    ----------
    filename: str
              Name of the h5d file as input.
    mode:     int
              mode in BaseInputProcessor
              0: default, when the file does not cover the entire simulation,
                 inputs default to 0.
              1: when the file does not cover the entire simulation,
                 inputs defaults to the last state.
    cache_length: int
                  length (number of steps) of cache on GPU memory to preload
                  inputs.

    Examples
    --------
    Input h5d must have the following format:
    1. For 'spike_state', there are two options. First,
        '/spike_state/uids': numpy array of dtype 'S',
                             contains the uids of the nodes recorded,
                             the order of this array determines the index below
        '/spike_state/data/time': numpy array of np.float32/np.double,
                                  contains a **monotonically** increasing time stamps
                                  of spikes from all nodes
        '/spike_state/data/index': numpy array of np.int32
                                   contains the index of every entry of spike time
                                   in 'spike_state/data/time'
        The output file generated by FileOutputProcessor complies with this requirement.

       A second option is to have it as the same format as in 2 below, but have
       only float/double value of 0 (no spike at the time step) or 1 (spike)

    2. For other variables, e.g., 'I':
        '/I/uids': numpy array of dtype 'S',
                  contains the uids of the nodes recorded,
                  the order of this array determines which column each node
                  is receives as input in the 'I/data' array.
        '/I/data': numpy array of dtype np.float32/np.double,
                   contains the value of inputs injected to the nodes.
    """
    def __init__(self, filename, mode = 0, cache_length = 1000):
        self.filename = filename
        self.cache_length = cache_length
        h5file = h5py.File(self.filename, 'r')
        var_list = []
        for var, g in h5file.items():
            if not isinstance(g, h5py.Group):
                continue
            uids = [a.decode('utf8') for a in g.get('uids')[()].tolist()]
            var_list.append((var, uids))
        super(FileInputProcessor, self).__init__(var_list, mode, memory_mode = 'gpu')
        h5file.close()

    def pre_run(self):
        self.h5file = h5py.File(self.filename, 'r')
        self.dsets = {}
        self.cache = {}
        self.counts = {}
        self.end_of_var_in_file = {}
        self.block_total = {}
        self.end_of_var = {}
        self.last_read_index = {}
        for var, g in self.h5file.items():
            if not isinstance(g, h5py.Group):
                continue
            self.block_total[var] = 0
            if var == 'spike_state':
                if 'index' in g.get('data'):
                    self.dsets[var] = {'index': g.get('data/index'),
                                       'time': g.get('data/time')}
                    self.spike_state_format = 'compact'
                else:
                    self.dsets[var] = g.get('data')
                    self.spike_state_format = 'full'
                self.cache[var] = garray.empty((self.cache_length, len(g['uids'])),
                                               self.variables[var]['input'].dtype)
            else:
                self.dsets[var] = g.get('data')
                self.cache[var] = garray.empty((self.cache_length, len(g['uids'])),
                                               self.variables[var]['input'].dtype)
            self.last_read_index[var] = 0
            self.counts[var] = 0
            self.end_of_var[var] = False
            self.end_of_var_in_file[var] = False
        # self.end_of_file = False

    def update_input(self):
        for var, dset in self.dsets.items():
            if self.counts[var] == self.block_total[var]:
                if not self.end_of_var_in_file[var]:
                    self.read_to_cache(var)

            if not self.end_of_var[var]:
                cuda.memcpy_dtod(self.variables[var]['input'].gpudata,
                                 int(self.cache[var].gpudata) + \
                                 self.cache[var].shape[1]*self.counts[var]*self.cache[var].dtype.itemsize,
                                 self.variables[var]['input'].nbytes)
                self.counts[var] += 1
            else:
                if not self.end_of_file:
                    self.variables[var]['input'].fill(0)

            if self.counts[var] == self.block_total[var]:
                if self.end_of_var_in_file[var]:
                    self.end_of_var[var] = True
        if self.end_of_file:
            self.h5file.close()

    def read_to_cache(self, var):
        if self.end_of_var_in_file[var]:
            # self.cache[var].fill(0)
            self.counts[var] = 0
            self.block_total[var] = self.cache_length
            return

        if var == 'spike_state':
            if self.spike_state_format == 'compact':
                self.read_spike_times(var)
            else:
                self.read_block_input(var)
        else:
            self.read_block_input(var)
        self.counts[var] = 0

    def read_block_input(self, var):
        tmp = self.dsets[var][self.last_read_index[var]:self.last_read_index[var]+self.cache_length,:]
        self.last_read_index[var] += tmp.shape[0]
        if self.last_read_index[var] == self.dsets[var].shape[0]:
            self.end_of_var_in_file[var] = True
        self.block_total[var] = tmp.shape[0]
        if tmp.shape[0] < self.cache_length:
            tmp1 = np.zeros(self.cache[var].shape, self.cache[var].dtype)
            tmp1[:tmp.shape[0]] = tmp
            tmp = tmp1
        self.cache[var].set(tmp)

    def read_spike_times(self, var):
        current_time = self._LPU_obj.time
        next_time = current_time + self.cache_length*self.sim_dt
        spike_times = []
        spike_index = []
        i = 0
        while True:
            tmp = self.dsets[var]['time'][self.last_read_index[var]:self.last_read_index[var]+10000]
            if tmp[0] >= next_time:
                break
            last_spike = tmp.shape[0] - np.argmax(tmp[::-1] < next_time)
            index = self.dsets[var]['index'][self.last_read_index[var]:self.last_read_index[var]+last_spike]
            spike_times.append(tmp[:last_spike])
            spike_index.append(index[:last_spike])
            self.last_read_index[var] += last_spike
            if self.last_read_index[var] == self.dsets[var]['time'].shape[0]:
                self.end_of_var_in_file[var] = True
                break
            if last_spike < tmp.shape[0]:
                break
        if len(spike_times):
            spike_times = np.concatenate(spike_times) - current_time
            spike_time_ids = np.floor(spike_times/self.sim_dt).astype(np.int32)
            spike_index = np.concatenate(spike_index)
            tmp = np.zeros(self.cache[var].shape, self.cache[var].dtype)
            self.block_total[var] = tmp.shape[0]
            np.add.at(tmp, (spike_time_ids, spike_index), 1)
            self.cache[var].set(tmp)
        else:
            self.cache[var].fill(0)

    @property
    def end_of_file(self):
        return all(self.end_of_var.values())

    def is_input_available(self):
        return not self.end_of_file

    def post_run(self):
        if not self.end_of_file:
            self.h5file.close()
